Now... how do we actually go about obtaining these vectors?


so... for configuration, maybe it makes sense to have a rigid kind of skeleton built up that
you expect things to conform to. The whole point of configuration is rigidity right

but for data I/O, I think you might need more flexibility. What does that mean/look like? Well,
I think maybe you could make a global list of output columns and their names. These basic
building blocks can be rigidly defined, I think. How to allocate their storage though?

so for data we simply need an enum for the internal GITR name and a pointer to a vector that
can be retrieved with that name. This can be the output data pool. Retrieve the pointer to the
vector from the pool manager - the lifeguard. Make it a unique_ptr.

/* map data to an int (all of it) */
class data_output_columns
{
  public:

  enum : int
  {
    name_of_data_column
  };

  /* this constructor will populate "lookup" */
  data_output_columns();

  shared_ptr get_vector( int );

  protected:

  /* Captain! Instead of a std::vector< double >, you may want to just do a T* and an int size */
  /* you're going to have to open a separate PR entirely for hashing. Start with just moving it
     into dedicated files. Then start figuring out everything in it */
  std::unordered_map< int, std::shared_ptr< std::vector< double > > > lookup;
};

/* map a name to an int */
class data_output_column_names
{
  public:

  /* populate "lookup" */
  data_output_column_names( data_output_columns );

  std::string get_name( int );

  protected:

  std::unordered_map< int, std::string > names;
};

/* both of the above classes might as well be singletons */

/* create a mapping from data columns to names */
class named_data_columns
{
  public:

  /* Captain! Verify that the one is completely contained in the other */
  /* I think columns should be contained in column names completely, but names could have more */
  named_data_columns( data_output_columns, data_output_column_names );

  /* throw exceptions depending on what is contained vs not in these */
  get_name();

  get_data_column();

  protected:

  data_output_columns;

  data_output_column_names;
};

/* write a mapping to a file - make a netcdf derived class and an hdf5 derived class for this */
class output_named_columns
{
  public:

  output_named_columns( named_data_columns );

  /* this can be either netcdf or hdf5 in an inherited method */
  virtual write_to_file();

  protected:

  named_data_columns;
};

Next, how can we incrementally move towards that from our current code??

read it to find out. Some version of lambdafying it I think will be necessary....

actually... what if we just directly make the jump. Then make two swap functions.

lambdafy it. then re-implement each lambda using other primitives? What should those be?

1. Lambda
2. Create class type for it.
3. Replace and recompile.
4. Re-implement as hdf5 and test

Captain! Not below here!

How can we organize these named data columns into identical output files consistent with the
current GITR interface? Create an abstract base class that aggregates data columns and writes
them to a named file with the correct names for the data classes.

/* map int to a string */
class jagged_data_matrix
{
  public:

  /* data and the name of the data */
  jagged_data_matrix( std::string file_name, data_columns );

  /* name of the file */
  get_file_name();

  /* will be further derived to use netcdf or hdf5 */
  /* simply calls the */
  void write_to_file()
  {
    /* simply needs to iterate through the "lookup" map to get the variable name and the
       data_columns map to get the data. Then just needs to call "write" on it? */
  }

  protected:

  std::unordered_map< int, std::string > lookup;

  std::shared_ptr< class data_output_columns > const data_columns;

  std::string name;
};

class geometry_hash final : public jagged_data_matrix
{
  public:

  /* populate "lookup". Pass args to base class */
  geometry_hash( std::string file_name, data_columns );
};


1. annotate
2. lambdafy
3. parameterize file names and data contents
4. fundamentally, you're bidirectionally mapping file names and data:
  
  file_name - different, runtime configurable. Load in a data class.

    get_name()
    protected string name in base class


  data_interface - different, preprocessor handled

  data - shared. Basic building block needs to be a pair of vector and string. call that a
         data_column

  abstract format - a list of data columns and a name for each list item, file name for each
  "jagged data matrix"

  link against a compiled library object that is implemented in a file data_file_io_netcdf
  that extends/implements an abstract base class defined in data_file_io target. This abstract
  base class must provide a mapping between internal GITR data and output files.

  separate base class and derived implemented classes into different file sets

  what kind of mapping do you want to use? Same idea as with the configuration interface I'd
  think. What does this entail? How will we map the file 


class geometry_hash : public jagged_data_matrix final
{
  public:

  enum int
  {
    col_name_0,
    col_name_1,
    col_name_2,
    ...
  };

  geometry_hash();


};


Captain! Graph the data flow through gitr.cpp. I think that is going to be the best way to
figure out what is going on in there.

Captain! Workflow after the examples are tested and verified: you need to have 2 windows,
of 2 panels each. One needs to be jerome's gitr and jerome.cpp
the other needs to be gitr.cpp and the other relevant files.
Copy it all over, then compile and test. If it passes, you're golden. Merge it in. By then,
hopefully you will have had a change to meet with Tim and ask

1. What is the deal with the input distribution grids for the surface models? According to
   what rules are they regridded and made into cdfs?

2. What is the meaning of the options and their values? Which ones are deprecated and can be
   removed?

3. Get some more examples that test all the options Tim tells you matter. Get these as PyGITR
   examples from Alyssa

4. Verify changes to the build options and the file interface. Switch to hdf5 file interface.
   Make netcdf optional and NOT enabled by default.

5. Merge all outstanding branches, tag a release.

6. Start the multi-species unit test.

7. Heavy refactor of source code.

8. Time dependence.

9. Geometry hashing stuff.

10. Automatic domain balancing

11. Interfacing with NDIP


What about integrating GITR stuff and runs into the data analysis? What if you used your
machines at home to set up a server and token issuer to do an ssh connection? Can you replicate
what they have on the site? Replicate their tech stack - you know it's there!

Also write detailed comments in user_options.cmake explaining what the options are and what the
different values for the options mean.

kubernetes

get aaron involved in the vis



Goals by tomorrow:

1. Get everything installed and building and running on fusiont6 for Atul tomorrow

2. Figure out how to make GITR generate the hashes.

3. Get the simple CPC example working. Use the hashes for that. Run it and get output. Run it
   3x: once on dev, once on your branch of netcdf. If they don't match, start running on
   intermediate commits.

4. Remove netcdf completely from GITR.

5. Integrate with the new build system. Compare outputs. Merge. Do runtime config.

Action plan:

1. Compile GITR on fusiont6
2. Finish the netcdf changes
3. Run the CPC example on fusiont5 while this is happening



This file will collect and explain all the steps

0. Important files:

   examples/cmod_meshing_gitr_erosion.m

     - this file creates a geometryPointPlane3D.cfg from an input stl file

     - also creates particle_source_cmod.nc

   examples/meshing/gitr_meshing_example.m

     - this file creates a geometryPointPlane3D.cfg from an input stl file


1. ADAS_Rates_Al.nc ---> material constants. Comes from known online data.

2. ftryDynSelf.nc ---> self-sputtering and redeposition coefficients/yields? Comes from ftrydyn

3. hashing file for geometry

4. hashing file for sheath

5. geometryPointPlane3D.cfg ---> generated from STL file by a matlab script
   Captain! This is in examples/meshing

6. gitrInput.cfg ---> this is the flags and options. Main config file.

7. helicon_processing.m ---> graphing script, not important

8.  particle_source_helicon.nc ---> particle sources.
    what is a particle source? Where they will be created. Initially? Location of eroded
    particles.

    How can you specify the location of eroded particles before the erosion occurs?

    simple assumption is that particle source covers the whole surface. Is it a 3d geometry
    laying on top of the surface?

    Particle source is a list of triangles that can produce impurity particles?

    In the largest case, it is all the triangles?

    In the efficient case, it is only some of the triangles that are significant?

    The red surface was every triangle.

    Each triangles has a surface normal?

    Question: why does geometryPointPlane3D not include the surface normals?

    Answer: because not all surface normals need to be calculated, only the ones that can
            potentially generate eroded impurity particles

    Which script produces particle_source_helicon.nc? Example script for the cmod case

    Captain! this is in examples/meshing/cmod_meshing_gitr_erosion.m


    Is it something like this:

    geometryPointPlane3D.cfg: triangle_0 ... triangle_1 ... triangle_2 ...

    particle_source_helicon:  [ 0, 2 ] (but not 1)

    figure out the other stuff from reading the scripts


9. profilesProtoMPEx.nc - what is this one? Magnetic fields, temp, density of the background
   plasma? Background plasma profiles. Stuff that isn't an impurity.

   2D profiles? Ah, they are extrapolated to 3D I think. 

   Is this information superimposed on top of geometryPointPlane3D.cfg?

   All data is on the geometry. So this is where BFIELD_INTERP reads from etc?

   where does this file come from? This comes from an external code like SOLPS.

   SOLPS outputs file_0, script_0 consumes file_0 and outputs profilesProtoMPEx.nc?

   next piece is find examples of file_0 and script_0. Not a single dedicated script but there
   probably could be

   currently, Atul has fake versions of file_0. What script_0 are you using?

   there is a github repo containing script_0, it is complicated.

   background plasma, temp, and density profiles can come from many different sources.

   just rely on an existing file

Questions we still have:

How exactly is hashing activated? One value for hashing causes it to be created as an output,
another causes it to be consumed as input. Which is which?


hashing files generated by a matlab script:




























